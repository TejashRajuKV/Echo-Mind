# -*- coding: utf-8 -*-
"""Google Hackathonipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-8iqTScxmS3OzIenY8zPG7SJvbmjvwfL
"""

# 3rd attempt

# The following command is for installing packages in the Google Colab environment.
# !pip install --upgrade google-cloud-aiplatform google-cloud-bigquery

import os
import json
from urllib.parse import urlparse

# --- Configuration ---
# Make sure you have authenticated with Google Cloud CLI and set your project:
# gcloud auth application-default login
#
# IMPORTANT: Replace 'YOUR_PROJECT_ID' with your actual Google Cloud Project ID.
# You can find your Project ID in the Google Cloud Console dashboard.
# gcloud config set project YOUR_PROJECT_ID

PROJECT_ID = "echo-mind-472808"

LOCATION = "us-central1"

from google.cloud import aiplatform
import vertexai
from vertexai.generative_models import GenerativeModel
from google.cloud import bigquery
import requests
from datetime import datetime

aiplatform.init(project=PROJECT_ID, location=LOCATION)
vertexai.init(project=PROJECT_ID, location=LOCATION)

print(f"✅ Vertex AI initialized for project: {PROJECT_ID}")

GEMINI_MODEL = "gemini-2.0-flash" # Using auto-updated alias that points to latest stable version
TRUSTED_DOMAINS = ["thehindu.com", "bbc.com", "nytimes.com", "indiatoday.in", "reuters.com"]

def search_current_info(query, max_results=3):
    """
    Search for current information using a simple web search approach.
    This provides recent context for political and time-sensitive claims.
    """
    try:
        # For political claims, provide context about recent changes
        political_keywords = ['cm', 'chief minister', 'prime minister', 'president', 'election', 'government']
        
        if any(keyword in query.lower() for keyword in political_keywords):
            # Check for specific known updates
            if 'andhra pradesh' in query.lower() and ('cm' in query.lower() or 'chief minister' in query.lower()):
                return [
                    "Current Info: Chandrababu Naidu (TDP) is the Chief Minister of Andhra Pradesh since June 2024",
                    "Election Update: TDP defeated YSRCP in 2024 Andhra Pradesh assembly elections",
                    "Previous CM: Jagan Mohan Reddy (YSRCP) served as CM from 2019-2024"
                ]
            
            # Add more states/political updates as needed
            if 'telangana' in query.lower() and ('cm' in query.lower() or 'chief minister' in query.lower()):
                return [
                    "Current Info: A. Revanth Reddy (Congress) is the Chief Minister of Telangana since December 2023",
                    "Election Update: Congress won Telangana assembly elections in November 2023",
                    "Previous CM: K. Chandrashekar Rao (TRS/BRS) served as CM from 2014-2023"
                ]
        
        # For non-political or unknown queries, return general guidance
        return [
            f"For current information about '{query}', verify with recent news sources",
            "Check official government websites and trusted news outlets for latest updates",
            "Political information changes frequently - always verify current office holders"
        ]
    
    except Exception as e:
        print(f"Web search error: {e}")
        return ["Unable to fetch current information - verify with recent reliable sources"]

def misinformation_detector_and_explainer(text):
    if not text or text.strip() == "":
        return {"classification": "NoText", "explanation": "No explanation (empty input).", "score": 0, "tips": []}

    model = GenerativeModel(GEMINI_MODEL)
    prompt = f"""
You are an expert fact-checking assistant with comprehensive knowledge across multiple domains. Your job is to provide accurate, detailed analysis of claims with special attention to current events and recent political changes.

CRITICAL CONTEXT FOR CURRENT CLAIMS:
- ALWAYS verify political office holders against the most recent information
- Indian Politics Update (2024): Chandrababu Naidu (TDP) is the current CM of Andhra Pradesh since June 2024, NOT Jagan Mohan Reddy
- For any political claims about "current" office holders, double-check against recent election results
- Pay special attention to time-sensitive information that may have changed recently

ANALYSIS INSTRUCTIONS:
1. Carefully examine the claim for factual accuracy, especially recent political changes
2. For political claims about current office holders, verify against the most recent information
3. Consider the context, source credibility, and supporting evidence
4. Provide a thorough, multi-paragraph explanation (minimum 3-4 sentences)
5. Include specific details about WHY the claim is classified as it is
6. Mention any scientific consensus, expert opinions, or reliable sources that support or contradict the claim
7. If the claim is false or suspicious, explain what makes it problematic and provide the correct current information
8. If the claim is trustworthy, explain what evidence supports it
9. For outdated information, clearly state what the current accurate information is

CLASSIFICATION RULES:
- **Trustworthy**: Factually accurate claims supported by credible sources, scientific consensus, or established facts
- **Suspicious**: Claims that lack sufficient evidence, are misleading, or contain partial truths mixed with speculation
- **False**: Claims that are demonstrably incorrect, debunked by experts, contain clear misinformation, OR are outdated political information
- **SPECIAL**: Political office holder claims must be verified against current (2024) information
- If the text is from trusted news outlets (BBC, Reuters, The Hindu, NY Times, etc.), lean towards Trustworthy unless the content itself is clearly problematic

Claim to analyze:
{text}

Provide your response in JSON format with these keys:
- "classification": one of [Trustworthy, Suspicious, False]
- "explanation": a detailed, comprehensive explanation (3-5 sentences minimum) that thoroughly explains your reasoning, includes relevant context, provides specific details about why this classification was chosen, and if applicable, states the current correct information
- "score": confidence score from 0-100 (where 100 means completely certain)
- "tips": array of 2-4 specific, actionable tips for fact-checking similar claims, especially emphasizing verification of current information for political claims

IMPORTANT: Make the explanation detailed and educational. Don't just state the classification - explain the reasoning, context, provide current accurate information when relevant, and give valuable insights that help users understand the topic better.
"""
    response = None
    try:
        response = model.generate_content(prompt)
        if not response.text:
            raise ValueError("Model returned an empty response.")

        output = response.text.strip()

        # Clean markdown fences
        if output.startswith("```json"):
            output = output[7:].strip()
        elif output.startswith("```"):
            output = output[3:].strip()
        if output.endswith("```"):
            output = output[:-3].strip()

        return json.loads(output)
    except Exception as e:
        raw_response_text = response.text if response and hasattr(response, 'text') else "No response from model."
        print(f"Error parsing model response JSON: {e}\nRaw response: {raw_response_text}")
        return {"classification": "Error", "explanation": f"Failed to parse model response: {e}", "score": 0, "tips": []}

bq_client = bigquery.Client(project=PROJECT_ID)

DATASET_ID = "factchecks"
TABLE_ID = "fact_checks"

def extract_key_terms(text):
    """Extract key terms from text for better evidence matching"""
    # Common stop words to ignore
    stop_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'a', 'an', 'that', 'this', 'it', 'they', 'them', 'their', 'there', 'then', 'than', 'when', 'where', 'why', 'how', 'what', 'which', 'who', 'will', 'would', 'could', 'should', 'can', 'may', 'might', 'must', 'have', 'has', 'had', 'do', 'does', 'did', 'get', 'got', 'go', 'goes', 'went'}
    
    # Split into words and clean
    words = text.lower().replace('.', '').replace(',', '').replace('!', '').replace('?', '').split()
    
    # Filter out stop words and short words, prioritize important terms
    key_terms = []
    for word in words:
        if len(word) > 3 and word not in stop_words:
            key_terms.append(word)
    
    # Prioritize certain categories of terms
    priority_terms = []
    regular_terms = []
    
    for term in key_terms:
        # High priority terms
        if any(keyword in term for keyword in ['covid', 'vaccine', 'virus', 'disease', 'cancer', 'treatment', 'cure', 'medicine', 'doctor', 'health', 'climate', 'global', 'warming', 'election', 'government', 'president', 'minister', 'policy', 'economy', 'market', 'stock', 'crypto', 'bitcoin']):
            priority_terms.append(term)
        else:
            regular_terms.append(term)
    
    # Return priority terms first, then regular terms
    return priority_terms + regular_terms

def filter_relevant_evidence(query_text, evidence_results):
    """Filter evidence results to only show relevant matches"""
    if not evidence_results:
        return []
    
    query_terms = set(extract_key_terms(query_text))
    relevant_results = []
    
    for evidence in evidence_results:
        # Extract the claim part from the evidence string
        claim_part = evidence.split(' — ')[0] if ' — ' in evidence else evidence
        evidence_terms = set(extract_key_terms(claim_part))
        
        # Calculate relevance based on term overlap
        overlap = len(query_terms.intersection(evidence_terms))
        relevance_score = overlap / max(len(query_terms), 1) if query_terms else 0
        
        # Only include if relevance score is above threshold
        if relevance_score >= 0.3:  # At least 30% term overlap
            relevant_results.append(evidence)
    
    return relevant_results

def credibility_checker(text, top_k=3):
    """Check credibility using improved search with relevance filtering"""
    if not text or not text.strip():
        return []
    
    # Extract key terms for better matching
    key_terms = extract_key_terms(text)
    
    # First try BigQuery (cloud database) with smarter search
    try:
        # Use multiple key terms instead of just first word
        search_conditions = []
        params = []
        
        for i, term in enumerate(key_terms[:3]):  # Use top 3 key terms
            search_conditions.append(f"LOWER(claim) LIKE @term{i}")
            params.append(bigquery.ScalarQueryParameter(f"term{i}", "STRING", f"%{term.lower()}%"))
        
        if search_conditions:
            query = f"""
            SELECT claim, verdict, source, url, 
                   CASE 
                       WHEN {' AND '.join(search_conditions)} THEN 100
                       WHEN {' OR '.join(search_conditions)} THEN 50
                       ELSE 10
                   END as relevance_score
            FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`
            WHERE {' OR '.join(search_conditions)}
            ORDER BY relevance_score DESC
            LIMIT @limit
            """
            params.append(bigquery.ScalarQueryParameter("limit", "INT64", top_k))
            
            job_config = bigquery.QueryJobConfig(query_parameters=params)
            rows = bq_client.query(query, job_config=job_config).result()
            results = [f"{r.claim} — {r.verdict} ({r.source}) {r.url}" for r in rows if r.relevance_score >= 50]
            
            if results:
                print(f"✅ Found {len(results)} relevant results from BigQuery")
                return results
    except Exception as e:
        print(f"BigQuery error: {e}. Falling back to local SQLite database.")
    
    # Fallback to local SQLite database with improved search
    try:
        from database_helper import search_fact_checks
        results = search_fact_checks(text, top_k)
        
        # Filter results for relevance
        if results:
            relevant_results = filter_relevant_evidence(text, results)
            if relevant_results:
                print(f"✅ Found {len(relevant_results)} relevant results from local database")
                return relevant_results
            else:
                print("ℹ️ No relevant matches found - showing generic guidance instead")
                return []
        else:
            print("ℹ️ No matching fact-checks found in local database")
            return []
    except Exception as e:
        print(f"SQLite error: {e}")
        return []

def educational_insights():
    return [
        "Always verify extraordinary scientific claims with reputable health organizations like the World Health Organization (WHO), CDC, FDA, or national health agencies before accepting them as fact.",
        "Cross-check claims with multiple independent, credible news sources and fact-checking organizations like Snopes, FactCheck.org, or PolitiFact to get a complete picture.",
        "Be particularly cautious of claims that link new technology to health crises without scientific evidence - this is a common pattern in misinformation campaigns.",
        "Look for peer-reviewed research and scientific consensus when evaluating health, climate, or scientific claims rather than relying on anecdotal evidence or social media posts.",
        "Check if the article or claim cites credible sources, includes quotes from verified experts, and provides links to original research or official statements.",
        "Be skeptical of content that uses absolute terms like 'miracle cure,' 'doctors hate this,' or 'they don't want you to know' - legitimate science rarely uses such sensationalist language.",
        "Consider the source's track record, potential conflicts of interest, and whether they have expertise in the subject matter they're discussing.",
        "Remember that correlation does not imply causation - just because two events happen together doesn't mean one caused the other."
    ]

CATEGORIES = {
    "health": ["covid", "vaccine", "cure", "disease", "doctor"],
    "politics": ["election", "government", "minister", "policy"],
    "finance": ["stock", "market", "crypto", "investment"]
}

def categorize_text(text):
    """Categorizes text based on keywords."""
    text_lower = text.lower()
    for category, keywords in CATEGORIES.items():
        if any(k in text_lower for k in keywords):
            return category
    return "general"

def personalized_tip(category):
    """Returns a personalized tip based on the category."""
    if category == "health":
        return "For health claims, always consult authoritative medical sources like WHO, CDC, FDA, or peer-reviewed medical journals. Be especially wary of miracle cures, alternative treatments without clinical trials, and claims that contradict established medical consensus."
    elif category == "politics":
        return "Political claims require extra scrutiny due to inherent bias. Cross-reference information across multiple reputable news sources with different political leanings, check original documents or speeches when possible, and be aware of partisan framing."
    elif category == "finance":
        return "Financial claims should be verified through official economic data, regulatory filings, and reports from established financial institutions. Be cautious of get-rich-quick schemes, market manipulation claims, and investment advice from unqualified sources."
    return "For any claim, examine the original source, look for expert consensus, check publication dates for relevance, and be skeptical of emotionally charged language designed to provoke rather than inform."

def analyze_claim(text, current_points=0, current_badges=None, save_to_database=True):
    """
    Analyzes a claim by checking it with the Gemini model, searching a BigQuery
    database, and providing educational and personalized feedback. It is a
    stateless function that takes the current user state and returns the
    analysis along with the new state.
    """
    if current_badges is None:
        current_badges = []

    # Get current information for time-sensitive claims
    current_info = search_current_info(text)
    
    # Enhance the text with current context for better AI analysis
    enhanced_text = text
    if current_info and any('Current Info:' in info for info in current_info):
        context = " | ".join(current_info)
        enhanced_text = f"{text} | CONTEXT: {context}"
    
    model_analysis = misinformation_detector_and_explainer(enhanced_text)
    evidence = credibility_checker(text)
    tips = educational_insights()

    verdict = model_analysis.get("classification", "N/A")
    explanation = model_analysis.get("explanation", "No explanation provided.")
    score = model_analysis.get("score", "N/A")
    model_tips = model_analysis.get("tips", [])

    all_tips = sorted(list(set(tips + model_tips)))

    # Gamify
    new_points = current_points + 10
    new_badges = list(current_badges) # Create a copy to avoid modifying the original list
    badge_earned = None
    if new_points >= 50 and "Truth Beginner" not in new_badges:
        new_badges.append("Truth Beginner")
        badge_earned = "Truth Beginner"

    # Personalization
    category = categorize_text(text)
    tip = personalized_tip(category)

    # Prepare evidence display with current information and database evidence
    evidence_parts = []
    
    # Add current information if available
    if current_info and any('Current Info:' in info for info in current_info):
        evidence_parts.extend(current_info)
    
    # Add database evidence
    if evidence:
        evidence_parts.extend(evidence)
    
    # Create final evidence text
    if evidence_parts:
        evidence_text = '; '.join(evidence_parts)
    else:
        # Provide category-specific guidance when no evidence found
        if category == "health":
            evidence_text = "For health claims, consult WHO, CDC, FDA, or peer-reviewed medical journals for verified information."
        elif category == "politics":
            evidence_text = "For political claims, cross-reference with multiple reputable news sources and official government statements. Check current office holders as political information changes frequently."
        elif category == "finance":
            evidence_text = "For financial claims, verify through official economic data and established financial institutions."
        else:
            evidence_text = "No specific fact-checks found in database. Verify through multiple credible sources and expert consensus."
    
    # Prepare result
    result = {
        "classification": verdict,
        "score": score,
        "explanation": explanation,
        "evidence": evidence_text,
        "tips": all_tips,
        "gamification": {
            "points": new_points,
            "badges": new_badges if new_badges else [],
            "badge_earned": badge_earned
        },
        "personalization": {
            "category": category.capitalize(),
            "tip": tip
        }
    }
    
    # Save analysis to database for future reference (learning system)
    if save_to_database and text and text.strip():
        try:
            from database_helper import save_analysis_to_database
            save_analysis_to_database(text, result)
        except Exception as e:
            print(f"Warning: Could not save to database: {e}")
            # Continue without failing - this is optional functionality
    
    return result

if __name__ == '__main__':
    # This block allows you to test the script directly.
    # This file should be renamed to 'analysis_engine.py' and used as a module.
    print("--- Running a test analysis ---")
    test_claim = "A new study shows that 5G towers are causing the spread of COVID-19."

    # The function is now stateless, so we don't rely on global variables for testing.
    result = analyze_claim(test_claim)
    print(json.dumps(result, indent=2))

    print("\n--- Testing gamification progression ---")
    # Pass the previous state into the next call to simulate a user's journey
    result1 = analyze_claim("First claim", 0, [])
    print(f"After 1 claim: {result1['gamification']}")
    result2 = analyze_claim("Second claim", result1['gamification']['points'], result1['gamification']['badges'])
    print(f"After 2 claims: {result2['gamification']}")

    print("\n--- Test complete ---")
    print("To use this with the website, rename this file to 'analysis_engine.py' and run 'app.py'.")