# -*- coding: utf-8 -*-
"""Google Hackathonipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-8iqTScxmS3OzIenY8zPG7SJvbmjvwfL
"""

# 3rd attempt

# The following command is for installing packages in the Google Colab environment.
# !pip install --upgrade google-cloud-aiplatform google-cloud-bigquery

import os
import json
from urllib.parse import urlparse

# --- Configuration ---
# Make sure you have authenticated with Google Cloud CLI and set your project:
# gcloud auth application-default login
#
# IMPORTANT: Replace 'YOUR_PROJECT_ID' with your actual Google Cloud Project ID.
# You can find your Project ID in the Google Cloud Console dashboard.
# gcloud config set project YOUR_PROJECT_ID

PROJECT_ID = "echo-mind-472808"

LOCATION = "us-central1"

from google.cloud import aiplatform
import vertexai
from vertexai.generative_models import GenerativeModel
from google.cloud import bigquery

aiplatform.init(project=PROJECT_ID, location=LOCATION)
vertexai.init(project=PROJECT_ID, location=LOCATION)

print(f"✅ Vertex AI initialized for project: {PROJECT_ID}")

GEMINI_MODEL = "gemini-2.0-flash" # Using the latest stable model for web/Gradio responses
TRUSTED_DOMAINS = ["thehindu.com", "bbc.com", "nytimes.com", "indiatoday.in", "reuters.com"]

def misinformation_detector_and_explainer(text):
    if not text or text.strip() == "":
        return {"classification": "NoText", "explanation": "No explanation (empty input).", "score": 0, "tips": []}

    model = GenerativeModel(GEMINI_MODEL)
    prompt = f"""
You are an expert fact-checking assistant with deep knowledge across multiple domains. Your job is to provide comprehensive, detailed analysis of claims.

ANALYSIS INSTRUCTIONS:
1. Carefully examine the claim for factual accuracy
2. Consider the context, source credibility, and supporting evidence
3. Provide a thorough, multi-paragraph explanation (minimum 3-4 sentences)
4. Include specific details about WHY the claim is classified as it is
5. Mention any scientific consensus, expert opinions, or reliable sources that support or contradict the claim
6. If the claim is false or suspicious, explain what makes it problematic and what the correct information is
7. If the claim is trustworthy, explain what evidence supports it

CLASSIFICATION RULES:
- **Trustworthy**: Factually accurate claims supported by credible sources, scientific consensus, or established facts
- **Suspicious**: Claims that lack sufficient evidence, are misleading, or contain partial truths mixed with speculation
- **False**: Claims that are demonstrably incorrect, debunked by experts, or contain clear misinformation
- If the text is from trusted news outlets (BBC, Reuters, The Hindu, NY Times, etc.), lean towards Trustworthy unless the content itself is clearly problematic

Claim to analyze:
{text}

Provide your response in JSON format with these keys:
- "classification": one of [Trustworthy, Suspicious, False]
- "explanation": a detailed, comprehensive explanation (3-5 sentences minimum) that thoroughly explains your reasoning, includes relevant context, and provides specific details about why this classification was chosen
- "score": confidence score from 0-100 (where 100 means completely certain)
- "tips": array of 2-4 specific, actionable tips for fact-checking similar claims

IMPORTANT: Make the explanation detailed and educational. Don't just state the classification - explain the reasoning, context, and provide valuable insights that help users understand the topic better.
"""
    response = None
    try:
        response = model.generate_content(prompt)
        if not response.text:
            raise ValueError("Model returned an empty response.")

        output = response.text.strip()

        # Clean markdown fences
        if output.startswith("```json"):
            output = output[7:].strip()
        elif output.startswith("```"):
            output = output[3:].strip()
        if output.endswith("```"):
            output = output[:-3].strip()

        return json.loads(output)
    except Exception as e:
        raw_response_text = response.text if response and hasattr(response, 'text') else "No response from model."
        print(f"Error parsing model response JSON: {e}\nRaw response: {raw_response_text}")
        return {"classification": "Error", "explanation": f"Failed to parse model response: {e}", "score": 0, "tips": []}

bq_client = bigquery.Client(project=PROJECT_ID)

DATASET_ID = "factchecks"
TABLE_ID = "fact_checks"

def credibility_checker(text, top_k=3):
    """Check credibility using BigQuery first, fall back to SQLite database"""
    if not text or not text.strip():
        return []
    
    # First try BigQuery (cloud database)
    try:
        token = text.split()[0].lower().replace("'", "").replace('"', '')
        query = f"""
        SELECT claim, verdict, source, url
        FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`
        WHERE LOWER(claim) LIKE @token
        LIMIT @limit
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ScalarQueryParameter("token", "STRING", f"%{token}%"),
                bigquery.ScalarQueryParameter("limit", "INT64", top_k),
            ]
        )
        rows = bq_client.query(query, job_config=job_config).result()
        results = [f"{r.claim} — {r.verdict} ({r.source}) {r.url}" for r in rows]
        if results:  # If BigQuery returns results, use them
            print(f"✅ Found {len(results)} results from BigQuery")
            return results
    except Exception as e:
        print(f"BigQuery error: {e}. Falling back to local SQLite database.")
    
    # Fallback to local SQLite database
    try:
        from database_helper import search_fact_checks
        results = search_fact_checks(text, top_k)
        if results:
            print(f"✅ Found {len(results)} results from local database")
            return results
        else:
            print("ℹ️ No matching fact-checks found in local database")
            return []
    except Exception as e:
        print(f"SQLite error: {e}")
        return []

def educational_insights():
    return [
        "Always verify extraordinary scientific claims with reputable health organizations like the World Health Organization (WHO), CDC, FDA, or national health agencies before accepting them as fact.",
        "Cross-check claims with multiple independent, credible news sources and fact-checking organizations like Snopes, FactCheck.org, or PolitiFact to get a complete picture.",
        "Be particularly cautious of claims that link new technology to health crises without scientific evidence - this is a common pattern in misinformation campaigns.",
        "Look for peer-reviewed research and scientific consensus when evaluating health, climate, or scientific claims rather than relying on anecdotal evidence or social media posts.",
        "Check if the article or claim cites credible sources, includes quotes from verified experts, and provides links to original research or official statements.",
        "Be skeptical of content that uses absolute terms like 'miracle cure,' 'doctors hate this,' or 'they don't want you to know' - legitimate science rarely uses such sensationalist language.",
        "Consider the source's track record, potential conflicts of interest, and whether they have expertise in the subject matter they're discussing.",
        "Remember that correlation does not imply causation - just because two events happen together doesn't mean one caused the other."
    ]

CATEGORIES = {
    "health": ["covid", "vaccine", "cure", "disease", "doctor"],
    "politics": ["election", "government", "minister", "policy"],
    "finance": ["stock", "market", "crypto", "investment"]
}

def categorize_text(text):
    """Categorizes text based on keywords."""
    text_lower = text.lower()
    for category, keywords in CATEGORIES.items():
        if any(k in text_lower for k in keywords):
            return category
    return "general"

def personalized_tip(category):
    """Returns a personalized tip based on the category."""
    if category == "health":
        return "For health claims, always consult authoritative medical sources like WHO, CDC, FDA, or peer-reviewed medical journals. Be especially wary of miracle cures, alternative treatments without clinical trials, and claims that contradict established medical consensus."
    elif category == "politics":
        return "Political claims require extra scrutiny due to inherent bias. Cross-reference information across multiple reputable news sources with different political leanings, check original documents or speeches when possible, and be aware of partisan framing."
    elif category == "finance":
        return "Financial claims should be verified through official economic data, regulatory filings, and reports from established financial institutions. Be cautious of get-rich-quick schemes, market manipulation claims, and investment advice from unqualified sources."
    return "For any claim, examine the original source, look for expert consensus, check publication dates for relevance, and be skeptical of emotionally charged language designed to provoke rather than inform."

def analyze_claim(text, current_points=0, current_badges=None):
    """
    Analyzes a claim by checking it with the Gemini model, searching a BigQuery
    database, and providing educational and personalized feedback. It is a
    stateless function that takes the current user state and returns the
    analysis along with the new state.
    """
    if current_badges is None:
        current_badges = []

    model_analysis = misinformation_detector_and_explainer(text)
    evidence = credibility_checker(text)
    tips = educational_insights()

    verdict = model_analysis.get("classification", "N/A")
    explanation = model_analysis.get("explanation", "No explanation provided.")
    score = model_analysis.get("score", "N/A")
    model_tips = model_analysis.get("tips", [])

    all_tips = sorted(list(set(tips + model_tips)))

    # Gamify
    new_points = current_points + 10
    new_badges = list(current_badges) # Create a copy to avoid modifying the original list
    badge_earned = None
    if new_points >= 50 and "Truth Beginner" not in new_badges:
        new_badges.append("Truth Beginner")
        badge_earned = "Truth Beginner"

    # Personalization
    category = categorize_text(text)
    tip = personalized_tip(category)

    # Return structured JSON data (as a dictionary)
    return {
        "classification": verdict,
        "score": score,
        "explanation": explanation,
        "evidence": '; '.join(evidence) if evidence else "No direct evidence found.",
        "tips": all_tips,
        "gamification": {
            "points": new_points,
            "badges": new_badges if new_badges else [],
            "badge_earned": badge_earned
        },
        "personalization": {
            "category": category.capitalize(),
            "tip": tip
        }
    }

if __name__ == '__main__':
    # This block allows you to test the script directly.
    # This file should be renamed to 'analysis_engine.py' and used as a module.
    print("--- Running a test analysis ---")
    test_claim = "A new study shows that 5G towers are causing the spread of COVID-19."

    # The function is now stateless, so we don't rely on global variables for testing.
    result = analyze_claim(test_claim)
    print(json.dumps(result, indent=2))

    print("\n--- Testing gamification progression ---")
    # Pass the previous state into the next call to simulate a user's journey
    result1 = analyze_claim("First claim", 0, [])
    print(f"After 1 claim: {result1['gamification']}")
    result2 = analyze_claim("Second claim", result1['gamification']['points'], result1['gamification']['badges'])
    print(f"After 2 claims: {result2['gamification']}")

    print("\n--- Test complete ---")
    print("To use this with the website, rename this file to 'analysis_engine.py' and run 'app.py'.")